{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Usage Notes\n",
    "\n",
    " - Use `past` in GPT-2 computations: <br> `past` is the ouput of a GPT-2 `model()` call and also its input; put `past` of previous output into next input to save computations\n",
    " \n",
    " - logit(p) = log(p/1-p) -> sigmoid(logit(p)) = p <br> can just use softmax (equiv to sigmoid for multivariate) and log on top\n",
    " \n",
    " - use prob_tensor.gather(dim, index_tensor) to extract probabilities of occurred words from distributions (tensors must be same dimensionality!) <br> => seems like ouput of GPT-2 is already words' probs extracted (but then do sigmoid instead of softmax)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# remove when training\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# input_ids = torch.tensor(tokenizer.encode(\"Bomb, my cat is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "# outputs = model(input_ids, labels=input_ids)\n",
    "# loss, logits = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = logits.softmax(-1)\n",
    "word_probs = probs.gather(-1, input_ids.unsqueeze(-1))\n",
    "\n",
    "word_probs.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../w3c-emails/emails.pkl\", \"rb\") as handle:\n",
    "    emails = pickle.load(handle)\n",
    "    \n",
    "mail_bodies_raw = [e.body_raw for e in emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ids = [torch.tensor(tokenizer.encode(body_str, add_special_tokens=True)) \n",
    "           for body_str in tqdm(mail_bodies_raw)]\n",
    "    \n",
    "    with open(\"GPT2_email_token_ids.pkl\", \"wb\") as handle:\n",
    "        pickle.dump(ids, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove when training\n",
    "with torch.no_grad():\n",
    "    for body_str in tqdm(mail_bodies_raw[:1]):\n",
    "        body_ids = torch.tensor(tokenizer.encode(body_str, add_special_tokens=True)).unsqueeze(0)\n",
    "        \n",
    "        print(body_ids.shape)\n",
    "        print(torch.tensor(tokenizer.encode(body_str, add_special_tokens=True)).shape)\n",
    "        \n",
    "        break\n",
    "        \n",
    "        loss, logits, _ = model(body_ids, labels=body_ids)#, past=past)\n",
    "        \n",
    "#         probs = logits.softmax(-1)\n",
    "#         word_probs = probs.gather(-1, body_ids.unsqueeze(-1))\n",
    "#         email_prob = word_probs.prod()\n",
    "        probs = logits.softmax(-1).log2()\n",
    "        word_probs = probs.gather(-1, body_ids.unsqueeze(-1))\n",
    "        email_prob = word_probs.sum()\n",
    "\n",
    "        \n",
    "        print(\"------\")\n",
    "        print(email_prob)\n",
    "        print(word_probs.squeeze(-1)[0, :10])\n",
    "        print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(body_ids[0]))[:100])\n",
    "        print()\n",
    "        \n",
    "        \n",
    "#         # use either, depending on output of model\n",
    "#         probs = logits.softmax(-1).log(base=2)\n",
    "#         word_probs = logits.sigmoid().log()\n",
    "        \n",
    "#         # get email log_prob via chain rule and extract number\n",
    "#         sent_log_p = log_probs.sum(-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"emails_token_ids.pkl\", \"rb\") as handle:\n",
    "    ids = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = ids[0]\n",
    "\n",
    "print(first.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 512\n",
    "k = 20\n",
    "\n",
    "l = 738\n",
    "\n",
    "\n",
    "# (0, n), (n-k, n-k+n=2n-k), ()\n",
    "\n",
    "\n",
    "# (0, 512=1*512-0*20), ((512-20), (512-20)+512=2*512-20), (2*512-20-20=2*512-2*20, )\n",
    "\n",
    "\n",
    "def cut_up_overlap(tens, n=512, k=20):\n",
    "    l = tens.shape[-1]\n",
    "    \n",
    "    if l < n:\n",
    "        return [tens]\n",
    "    \n",
    "    for \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = 5\n",
    "k = 2\n",
    "l \n",
    "\n",
    "cut_up_overlap = []\n",
    "\n",
    "\n",
    "for i in range(4):    \n",
    "    print(\n",
    "        (i*n)-(i*k),\n",
    "        ((i+1)*n)-(i*k)\n",
    "    )\n",
    "    \n",
    "    print(list(range((i*n)-(i*k),\n",
    "        ((i+1)*n)-(i*k))))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# [(i, (i+1)*n) for i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 511\n",
    "k = 20\n",
    "\n",
    "l = 718\n",
    "\n",
    "x = torch.arange(0, l)\n",
    "\n",
    "unfd = x.unfold(0, n, n-k)\n",
    "\n",
    "got = unfd.shape[0]*unfd.shape[1] - unfd.shape[0]*(k)\n",
    "\n",
    "last = x[got:]\n",
    "\n",
    "tuple(v for v in unfd)+ (last,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "511+227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.split(511)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
