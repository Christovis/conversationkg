# Entities as Text
## (Entity Resolution as Authorship Attribution)

- core idea: let entities (persons, organisations, topics, etc.) be represented by the texts they produce <br>
    in our context, an entity is represented by the set of e-mails sent in its name <br>
    => entity resolution becomes equivalent to authorship attribution
    
- requires: 
   1. unified (optimally also fixed-length) representation of text
   2. similarity measure for representations
   
- 
   
- two approaches:
   1. probability-based: each entity is considered as a probabilistic source of text <br>
      by obtaining a _language model_ from the entity's texts and using Bayes' Theorem, we can assess the probability
      that a given text was generated by that entity (using MAP, i.e. $max_E P(E)P(t|E)$)
   2. embedding-based: 



## Task

- given two emails e_1 and e_2 from author _labels_ X and Y, decide whether X and Y are labels for the same entity <br>
  can be phrased as P(author(e_1) = author(e_2))

- in order perform entity resolution, we form/sample email pairs from all/some labels X and Y and merge the labels if the emails pair has high probability of being from the same author (labels becomes aliases) <br>
    advantage: 
 
## Data

- need pairs of e-mails to train a 


## Models

- all models first compute a similarity score that is then transformed into a probability by the sigmoid function


### Baselines

 1. Jaccard Similarity: arguably a most naive baseline; no learning involved, i.e. no parameters <br>
    given emails e_1 and e_2 simply computes the multiset Jaccard similarity between them
   
 2. averaged BERT embeddings: given a tokenised text, the pre-trained BERT model returns embeddings for each token in the text
    - this baseline computes the average embedding of the tokens in e_1 and e_2, respectively, and then the cosine similarity of the averages
    - however: [1] demonstrate that averaged BERT embeddings do not perform better than averaged GloVe embeddings and [2] explain why BERT does not directly allow for language model interpretations
    - moreover, initial visualisations of the embedding space suggest that true positives are not likely detected by this model (embeddings of emails from the same author labels are uniformly spread across the embedding space)
    
    
### Classifier BERT

 - instead of averaging, train an LSTM to aggregate the embeddings from BERT into a single vector
 - the LSTM is explicitly trained s.t. emails from the same label are embedded into vectors with high cosine similarity
    
    
## Results


 - classifier scoring metrics: F1, accuracy, balanced accuracy (accounts for uneven class proportions), precision, recall
 

   | model     | threshold |   |  F1  | accuracy | balanced accracy | precision | recall |
   |-----------|:---------:|---|:----:|:--------:|:----------------:|:---------:|:------:|
   | Jaccard   |    0.15   |   | 0.69 |   0.82   |       0.77       |    0.72   |  0.66  |
   | Jaccard   |    0.18   |   | 0.67 |   0.83   |       0.76       |    0.81   |  0.57  |
   |           |           |   |      |          |                  |           |        |
   | avg BERT  |    0.90   |   | 0.63 |   0.71   |       0.75       |    0.51   |  0.83  |
   | avg BERT  |    0.93   |   | 0.63 |   0.76   |       0.74       |   0.59    |  0.69  |
   |           |           |   |      |          |                  |           |        |
   | BERT+LSTM |    0.76   |   | 0.43 |   0.67   |       0.60       |   0.44    |  0.42  |
   | BERT+LSTM |    0.90   |   | 0.31 |   0.70   |       0.56       |   0.49    |  0.23  |
   
   
 - receiver operating characteristic (ROC) and area under the curve (AUC) 
 
   1. Jaccard similarity <br>
   
   ![ROC of Jaccard](https://github.com/pgroth/conversationkg/blob/master/embeddings_for_the_people/classifier_BERT/results/images/ROC_Jaccard.png)

   2. Averaged BERT <br>
   
   ![ROC of avg BERT](https://github.com/pgroth/conversationkg/blob/master/embeddings_for_the_people/classifier_BERT/results/images/ROC_avg_BERT.png)
  
  3. BERT+LSTM <br>
  
  ![ROC of BERT+LSTM](https://github.com/pgroth/conversationkg/blob/master/embeddings_for_the_people/classifier_BERT/results/images/ROC_LSTM%2BBERT.png)



 - embedding space visualisation



## Conclusions

 - Jaccard similarity is a very strong baseline, could actually provide a useful model <br>
   -> shows that task is feasible in principle (additional heuristics could further improve performance)
 - fact that averaging BERT embeddings is better than 



[1]: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (https://arxiv.org/pdf/1908.10084.pdf)

[2]: Language Models with Transformers (https://arxiv.org/pdf/1904.09408.pdf)
