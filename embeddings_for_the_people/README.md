# Entities as Text
## (Entity Resolution as Authorship Attribution)

- goal: decide whether two entity _labels_ identify the same entity (could be due to spelling, fraud, context)

- core idea: let entities (persons, organisations, topics, etc.) be represented by the texts they produce; <br>
    in our context, an entity is represented by the set of e-mails sent in its name <br>
    => entity resolution becomes equivalent to authorship attribution
    
- requires: 
   1. unified (optimally also fixed-length) representation of text
   2. similarity measure for representations
   
   
- two approaches:
   1. probability-based: each entity is considered as a probabilistic source of text; <br>
      by obtaining a _language model_ from the entity's texts and using Bayes' Theorem, we can assess the probability
      that a given text was generated by that entity (using MAP, i.e. $max_E P(E)P(t|E)$)
   2. embedding-based: emails are projected into an embedding space, where they can be compared; <br>
      the probability that emails have the same author becomes equivalent to the similarities of their embeddings; <br>
      task becomes finding an appropriate function to project into the embedding space
     
- both approaches benefit from pre-trained models that can inject open-world knowledge; <br>
  recent advances with transformer-based neural networks have shown impressive ability of capturing precisely this <br>
  hence, use pre-trained transformers as basis of models


## Task

- basic assumption: if entity _labels_ X and Y for emails e_1 and e_2 are the same, then e_1 and e_2 _really are_ from the same author <br>
  => such data are positive samples
  
- observation: if we randomly sample two emails e_1 and e_2, then with overwhelming probability, they are _not_ from the same author (regardless of the entity _labels_ of e_1 and e_2)

- task: given emails e_1 and e_2, with _different labels_ X and Y, decide whether e_1 and e_2 are from the same author <br>
  (i.e. the labels X and Y identify the same entity; <br>
  can be phrased as P(author(e_1) = author(e_2))

- in order perform entity resolution, we form/sample email pairs from all/some labels X and Y and merge the labels if the emails pair has high probability of being from the same author (labels becomes aliases) <br>
    advantage: if X and Y each have multiple emails associated with them, then we can rely voting strategies to increase confidence and reliability
 
 
## Data

- positive samples: from the set of emails associated with a label, sample pairs of emails; <br>
  cf. the basic assumption

- negative samples: from the set of all emails in the corpus, sample a set of pairs

- 


- positive-negative ratio in the training data is 30/70; <br>
  classifier should be aware that in most cases, two random emails are _not_ from the same author


## Models

- all models first compute a similarity score that is then transformed into a probability by the sigmoid function


### Baselines

 1. Jaccard Similarity: arguably a most naive baseline; no learning involved, i.e. no parameters <br>
    given emails e_1 and e_2 simply computes the multiset Jaccard similarity between them
   
 2. averaged BERT embeddings: given a tokenised text, the pre-trained BERT model returns embeddings for each token in the text
    - this baseline computes the average embedding of the tokens in e_1 and e_2, respectively, and then the cosine similarity of the averages
    - however: [1] demonstrate that averaged BERT embeddings do not perform better than averaged GloVe embeddings and [2] explain why BERT does not directly allow for language model interpretations
    - moreover, initial visualisations of the embedding space suggest that true positives are not likely detected by this model (embeddings of emails from the same author labels are uniformly spread across the embedding space)
    
    
### Classifier BERT

 - instead of averaging, train an LSTM to aggregate the embeddings from BERT into a single vector
 - the LSTM is explicitly trained s.t. emails from the same label are embedded into vectors with high cosine similarity
    
    
## Results


 - classifier scoring metrics: F1, accuracy, balanced accuracy (accounts for uneven class proportions), precision, recall
 

   | model     | threshold |   |  F1  | accuracy | balanced accuracy | precision | recall |
   |-----------|:---------:|---|:----:|:--------:|:----------------:|:---------:|:------:|
   | Jaccard   |    0.15   |   | 0.69 |   0.82   |       0.77       |    0.72   |  0.66  |
   | Jaccard   |    0.18   |   | 0.67 |   0.83   |       0.76       |    0.81   |  0.57  |
   |           |           |   |      |          |                  |           |        |
   | avg BERT  |    0.90   |   | 0.63 |   0.71   |       0.75       |    0.51   |  0.83  |
   | avg BERT  |    0.93   |   | 0.63 |   0.76   |       0.74       |   0.59    |  0.69  |
   |           |           |   |      |          |                  |           |        |
   | BERT+LSTM |    0.76   |   | 0.43 |   0.67   |       0.60       |   0.44    |  0.42  |
   | BERT+LSTM |    0.90   |   | 0.31 |   0.70   |       0.56       |   0.49    |  0.23  |
   
   <br><br>
   
   
 - receiver operating characteristic (ROC) and area under the curve (AUC) 
 
   i. Jaccard similarity <br>
   
   ![ROC of Jaccard](https://github.com/pgroth/conversationkg/blob/master/embeddings_for_the_people/classifier_BERT/results/images/ROC_Jaccard.png)

   ii. Averaged BERT <br>
   
   ![ROC of avg BERT](https://github.com/pgroth/conversationkg/blob/master/embeddings_for_the_people/classifier_BERT/results/images/ROC_avg_BERT.png)
  
  iii. BERT+LSTM <br>
  
  ![ROC of BERT+LSTM](https://github.com/pgroth/conversationkg/blob/master/embeddings_for_the_people/classifier_BERT/results/images/ROC_LSTM%2BBERT.png)

   <br><br>
   
 - embedding space visualisation

 - histogram <br>
 
 ![Histogram](https://github.com/pgroth/conversationkg/blob/master/embeddings_for_the_people/classifier_BERT/results/images/negative_positive_histograms_LSTM%2BBERT.png)


## Conclusions

 - Jaccard similarity is a very strong baseline, could actually provide a useful model <br>
   -> shows that task is feasible in principle (additional heuristics could further improve performance)
 
 - fact that averaging BERT embeddings is better than the LSTM which is specifically tuned is puzzling <br>
   => probably something gone wrong during training <br>
   => BERT embeddings might perhaps obfuscate 
   



[1]: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (https://arxiv.org/pdf/1908.10084.pdf)

[2]: Language Models with Transformers (https://arxiv.org/pdf/1904.09408.pdf)