{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicClassifierCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=None, dropout=0.):\n",
    "        super(BasicClassifierCell, self).__init__()        \n",
    "        self.dout = nn.Dropout(dropout)\n",
    "        self.l = nn.Linear(2*input_size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        vec1, vec2 = inputs\n",
    "        concated = self.dout(torch.cat((vec1, vec2), -1))\n",
    "        return self.sig(self.l(concated))\n",
    "\n",
    "class ClassifierCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, dropout=0.):\n",
    "        super(ClassifierCell, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.dout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.l2 = nn.Linear(2*hidden_size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        vec1, vec2 = inputs\n",
    "        \n",
    "        a11 = self.relu1(self.l1(vec1))\n",
    "        a12 = self.relu1(self.l1(vec2))\n",
    "        concated = self.dout(torch.cat((a11, a12), -1))\n",
    "        \n",
    "        return self.sig(self.l2(concated))\n",
    "    \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, word_emb_size, lstm_hidden_size, lstm_num_layers, clssfr_cell_type, clssfr_hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.is_bidirectional = True\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_size, \n",
    "                            hidden_size=lstm_hidden_size, \n",
    "                            num_layers=lstm_num_layers,\n",
    "                            dropout=0.2, bidirectional=self.is_bidirectional)\n",
    "        \n",
    "        self.clssfr = clssfr_cell_type(lstm_hidden_size, \n",
    "                                     hidden_size=clssfr_hidden_size,\n",
    "                                     dropout=0.2)\n",
    "        \n",
    "    def encode(self, seq, h_0=None, c_0=None):\n",
    "        outs, (h_n, c_n) = self.lstm(seq)\n",
    "        \n",
    "        if self.is_bidirectional:\n",
    "            forward_out = outs[-1, :, :self.lstm_hidden_size]\n",
    "            backward_out = outs[0, :, self.lstm_hidden_size:]\n",
    "            return (forward_out+backward_out)/2\n",
    "        else:\n",
    "            return outs[-1]\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        seq1, seq2 = inputs        \n",
    "        enc1, enc2 = self.encode(seq1), self.encode(seq2)\n",
    "        return self.clssfr((enc1, enc2))\n",
    "    \n",
    "    \n",
    "    def fit(self, inputs, true_outputs, epochs=10):\n",
    "        # convert inputs & outputs to tensors (batch?)\n",
    "        # setup training: optimiser, loss func, train params\n",
    "        # keep track of variables: loss and prediction\n",
    "        # train loop\n",
    "        \n",
    "        loss_f = torch.nn.BCELoss()\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "\n",
    "        losses, preds = [], []\n",
    "\n",
    "        for i in tqdm(range(epochs)):\n",
    "            pred = self.forward((one_tens, two_tens))\n",
    "            preds.append(pred)\n",
    "    \n",
    "            loss = loss_f(pred, y)\n",
    "            losses.append(loss)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 768\n",
    "cls = Classifier(word_emb_size=d, lstm_hidden_size=7, lstm_num_layers=3, \n",
    "                 clssfr_cell_type=BasicClassifierCell, clssfr_hidden_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = torch.arange(float(d)).unsqueeze(0).unsqueeze(0), torch.arange(float(d)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# x, y = torch.randn(2,2,2), torch.randn(2,2,2)\n",
    "\n",
    "o = torch.autograd.Variable(torch.tensor([0.]))\n",
    "\n",
    "cls((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertModel\n",
    "import pickle\n",
    "\n",
    "# with open(\"emails_token_ids.pkl\", \"rb\") as handle:\n",
    "#     ids = pickle.load(handle)[:20]\n",
    "\n",
    "\n",
    "# bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "# bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_up(mail_tensor, n=512):\n",
    "    split_tens = mail_tensor.split(n)\n",
    "    \n",
    "    if split_tens[-1].nelement() == split_tens[0].nelement():\n",
    "        return torch.stack(split_tens), None\n",
    "    else:\n",
    "        return torch.stack(split_tens[:-1]), split_tens[-1].unsqueeze(0)\n",
    "\n",
    "\n",
    "# WRAPPER FOR MODEL CALL\n",
    "def email_to_vec(email_body_ids, to_id_first=False, chunk_size=512):\n",
    "    if to_id_first:\n",
    "        input_ids = email_to_ids(email_body_ids)\n",
    "    else:\n",
    "        input_ids = email_body_ids\n",
    "        \n",
    "    chunks, end_chunk = cut_up(input_ids, chunk_size)\n",
    "    \n",
    "#     chunks_cuda = chunks.to(device)\n",
    "    outputs, *_ = bert(chunks)\n",
    "    \n",
    "    outputs_flattened = outputs.view(-1, outputs.shape[-1])\n",
    "    \n",
    "    if end_chunk is not None:\n",
    "#         end_cuda = end_chunk.to(device)\n",
    "        end_output, *_ = bert(end_chunk)\n",
    "        outputs_flattened = torch.cat((outputs_flattened, end_output.squeeze(0)), 0)\n",
    "        \n",
    "    return outputs_flattened.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    first_twenty = [email_to_vec(mt) for mt in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"first_twenty.pkl\", \"rb\") as handle:\n",
    "    first_twenty = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    first = email_to_vec(ids[0])\n",
    "    second = email_to_vec(ids[1])\n",
    "    print(first.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tens = torch.tensor(first).unsqueeze(1)\n",
    "second_tens = torch.tensor(second).unsqueeze(1)\n",
    "\n",
    "print(first_tens.size())\n",
    "print(second_tens.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls((first_tens, second_tens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, one in enumerate(first_twenty):\n",
    "        print(i)\n",
    "        for two in first_twenty:\n",
    "            one_tens = torch.tensor(one).unsqueeze(1)\n",
    "            two_tens = torch.tensor(two).unsqueeze(1)\n",
    "\n",
    "\n",
    "            print(\"\\t\", cls((one_tens, two_tens)))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = torch.nn.BCELoss()\n",
    "optim = torch.optim.Adam(cls.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([[0.]])\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "losses, preds = [], []\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    pred = cls((one_tens, two_tens))\n",
    "    preds.append(pred)\n",
    "    \n",
    "    loss = loss_f(pred, y)\n",
    "    losses.append(loss)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
