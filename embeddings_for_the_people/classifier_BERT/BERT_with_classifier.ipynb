{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "## BERT on e-mails\n",
    "\n",
    " - run BERT on each e-mail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForMaskedLM, DistilBertConfig\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"emails_token_ids.pkl\", \"rb\") as handle:\n",
    "    ids = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(tokenizer.encode(email_body, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roadmap\n",
    "\n",
    "## encode e-mails with BERT\n",
    "  \n",
    "  - for each e-mail, $e=(w_1, ..., w_n)$, encode it with BERT:<br>\n",
    "    $BERT(e)= (BERT(w_1), ..., BERT(w_n))$\n",
    "    \n",
    "  - save in convenient format <br>\n",
    "    $\\rightarrow$ be careful that size doesn't explode <br>\n",
    "    $\\rightarrow$ how to load for training?\n",
    "    \n",
    "  - make sure that order correctly recoverable <br>\n",
    "    $\\rightarrow$ index e-mails for cheap and convenient flows during training?\n",
    "    \n",
    "## build classifier architecture\n",
    "\n",
    "  - LSTM: reads encoded e-mails (token by token) and outputs a single representation <br>\n",
    "    $\\rightarrow$ LSTM's output for last token (average of both outputs for bidirectional)\n",
    "    \n",
    "  - classifier: simple feedforward layer which pools LSTM's representation into single value to apply sigmoid over<br>\n",
    "    $\\rightarrow$ goal is to force the LSTM to produce useful representations, i.e. give the classifier as little power as possible <br>\n",
    "    \n",
    "  - train on e-mail tuples: either from same author ($y=1$) or not ($y=0$) <br>\n",
    "    $\\rightarrow$ take $e_1$ and $e_2$ from **same** author label as samples with $y=1$ <br>\n",
    "    $\\rightarrow$ take $e_1$ and $e_2$ from **different** author label as samples with $y=0$ \n",
    "\n",
    "    \n",
    "    \n",
    "## create data\n",
    "\n",
    "  - use snorkel's augmentation framework:\n",
    "    - sample positive and negative data\n",
    "    - use a mean field policy to create more negative than positive data \n",
    "    - each sample is `((e, e'), y)`\n",
    "    \n",
    "  - put data into pandas for convenience?  \n",
    "  \n",
    "  - translate:\n",
    "    - e-mails into indices (based on original order)\n",
    "    - corpus into a dict with `auth: [e_1, ..e_n]`\n",
    "    \n",
    "  - split into train, eval and test set <br>\n",
    "    $\\rightarrow$ based on characteristics of authors or e-mails?\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow\n",
    "    \n",
    "`emails.pkl` 笨能n",
    "\n",
    "$~~~~~~~~~~\\downarrow~~~~$ `BertTokenizer` 笨能n",
    "\n",
    "`emails_token_ids.pkl` 笨能n",
    "\n",
    "$~~~~~~~~~~\\downarrow~~~~$ `BertModel` \n",
    "\n",
    "`emails_encoded.pkl`\n",
    "\n",
    "$~~~\\downarrow~~$ $~~~\\downarrow~~~~~~$ &emsp;&emsp;&emsp;&emsp; $\\downarrow~~$`emails_to_data` 笨能n",
    "\n",
    "`train` `eval` &emsp;&emsp;&emsp;&emsp; `test`\n",
    "\n",
    "$~~~\\downarrow~~~~$ `train_cls` $~~~~~~~\\downarrow$\n",
    "\n",
    "`trained_model` $~~~~~~~~~~~~\\downarrow$\n",
    "\n",
    "$~~~\\downarrow~~~~$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\downarrow$ `test_classifier`\n",
    "\n",
    "`accuracy` and `other_scores`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
