{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 17:49:14.898231 140338297841472 file_utils.py:38] PyTorch version 1.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "import torch\n",
    "# from torch import nn\n",
    "import numpy as np\n",
    "perm = np.random.permutation\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load E-mail Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../w3c-emails/emails.pkl\", \"rb\") as handle:\n",
    "    emails = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    " 0. select authors (so that evaulation sets can be held out) and establish their frequencies <br>\n",
    "   -> $P(X)$, i.e. the probability that a person appears as an author of any e-mail\n",
    " 1. pre-train GPT-2 on W3C e-mail corpus <br> \n",
    "   -> W3CGPT-2 <br>\n",
    "   -> approximates $P(email)$\n",
    " 2. train W3CGPT-2 on e-mails by selected authors <br>\n",
    "   -> GPT-2$_X$ for each person $X$ <br> \n",
    "   -> i.e. $P(email|X)$\n",
    " 3. use GPT-2$_X$ to classify unseen (both to W3CGPT-2 and to GPT-2$_X$) e-mails <br>\n",
    "   -> $P(X|email) = P(X)P(email|X)/P(email)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Select Authors (and their e-mails)\n",
    "\n",
    " - author-frequency distribution Zipfian, so use log-linear ranks to select authors <br> \n",
    "   (i.e. author rank 1, author rank 2, author rank 4, author rank 8, ..., author rank 512)\n",
    "   \n",
    " - plots below equivalent to $P(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_ranks(emails, ls_of_ranks):\n",
    "    sndr_cnts = Counter(e.sender for e in emails)\n",
    "    ranks_sndr = {r:s for r, (s, c) in enumerate(sndr_cnts.most_common())}\n",
    "    \n",
    "    for r in ls_of_ranks:\n",
    "        cur_s = ranks_sndr[r]\n",
    "        yield [m for m in emails if m.sender == cur_s]\n",
    "        \n",
    "rank_rng = [2**i for i in range(10)]\n",
    "selection = list(select_by_ranks(emails, rank_rng))\n",
    "rest = list(set(emails) - set(m for m_ls in selection for m in m_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sndr_cnts = Counter(e.sender for e in emails)\n",
    "rs, cs = list(zip(*[(r, c) for r, (_, c) in enumerate(sndr_cnts.most_common())]))\n",
    "\n",
    "fig = plt.figure(1, figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.loglog(rs, cs, '.')\n",
    "plt.xlabel(\"$\\log$ rank\"); plt.ylabel(\"$\\log$ frequency\"); plt.title(\"Rank-Frequency plot of number of e-mails authored by each person\\n$= P(X)$\")\n",
    "\n",
    "rng = [2**i for i in range(10)]\n",
    "rng_cs = [cs[i] for i in rng]\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.loglog(rng, rng_cs, '.')\n",
    "_ = plt.title(\"Thinned version of the left plot\\n(subset of 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Get Train and Evaluation sets\n",
    "\n",
    "using train:eval ratio of 30:70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(ls, test_ratio=0.3):\n",
    "    cutoff = int(len(ls)*test_ratio)\n",
    "    randmsd_ls = list(perm(ls))\n",
    "    test = randmsd_ls[:cutoff]\n",
    "    train = randmsd_ls[cutoff:]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# def emails_to_datasets(selected, rest, test_ratio=0.3):\n",
    "#     train, test = split_train_test(rest, test_ratio)\n",
    "#     for mail_ls in selected:\n",
    "#         cur_train, cur_test = split_train_test(mail_ls, test_ratio)\n",
    "#         train.extend(cur_train)\n",
    "#         test.extend(cur_test)\n",
    "        \n",
    "#     return train, test\n",
    "\n",
    "selection_train, selection_test = list(zip(*[split_train_test(m_ls) for m_ls in selection]))\n",
    "rest_train, rest_test = split_train_test(rest)\n",
    "selection_never_seen, selection_test = list(zip(*[split_train_test(m_ls, test_ratio=0.5) \n",
    "                                                   for m_ls in selection_test]))\n",
    "rest_never_seen, rest_test = split_train_test(rest_test, test_ratio=0.5)\n",
    "print(list(map(len, selection_never_seen)), list(map(len, selection_test)))\n",
    "len(rest_never_seen), len(rest_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in folders because of random permutations\n",
    "\n",
    "with open(\"data_splits/selection_train.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(selection_train, handle)\n",
    "with open(\"data_splits/selection_test.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(selection_test, handle)\n",
    "with open(\"data_splits/selection_never_seen.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(selection_never_seen, handle)\n",
    "    \n",
    "\n",
    "with open(\"data_splits/rest_train.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(rest_train, handle)\n",
    "with open(\"data_splits/rest_test.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(rest_test, handle)\n",
    "with open(\"data_splits/rest_never_seen.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(rest_never_seen, handle)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START FROM HERE\n",
    "# IMPORTANT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_splits/selection_train.pkl\", \"rb\") as handle:\n",
    "    selection_train = pickle.load(handle)\n",
    "with open(\"data_splits/selection_test.pkl\", \"rb\") as handle:\n",
    "    selection_test = pickle.load(handle)\n",
    "with open(\"data_splits/selection_never_seen.pkl\", \"rb\") as handle:\n",
    "    selection_never_seen = pickle.load(handle)\n",
    "    \n",
    "\n",
    "with open(\"data_splits/rest_train.pkl\", \"rb\") as handle:\n",
    "    rest_train = pickle.load(handle)\n",
    "with open(\"data_splits/rest_test.pkl\", \"rb\") as handle:\n",
    "    rest_test = pickle.load(handle)\n",
    "with open(\"data_splits/rest_never_seen.pkl\", \"rb\") as handle:\n",
    "    rest_never_seen = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Domain-Adapt GPT-2 to W3C E-mails\n",
    "\n",
    " - pre-train an instance of GPT-2 on entire (subset of) w3c-email corpus <br>\n",
    "   -> will reduce perplexity and thus increase sensitivity of LMs\n",
    " - use this LM as starting point to train personalised LMs \n",
    " - also reserve a test set? -> i.e. some e-mails which no custom-trained LM has seen before\n",
    " \n",
    " - => write e-mail bodies into text files, train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emails_to_trainfile(email_ls, file_name, split_into=1):\n",
    "    chunk_size = len(email_ls)//split_into\n",
    "    for i in range(split_into):\n",
    "        with open(file_name + f\".{i}\", \"w\", encoding=\"utf-8\") as handle:\n",
    "            cur_chunk = email_ls[i*chunk_size:(i+1)*chunk_size] if i != split_into-1 else email_ls[i*chunk_size:]\n",
    "            for m in cur_chunk:\n",
    "                mail_str = m.body_raw.replace(\"\\n\", \"  \")\n",
    "                handle.write(mail_str)\n",
    "                handle.write(\"\\n\\n\")\n",
    "            \n",
    "full_train = perm(rest_train + \n",
    "                  [m for m_ls in selection_train for m in m_ls] +\n",
    "                  [m for m_ls in selection_test for m in m_ls])\n",
    "\n",
    "full_test = perm(rest_never_seen + [m for m_ls in selection_never_seen for m in m_ls])\n",
    "\n",
    "emails_to_trainfile(full_train, \"W3CGPT2/full.train.raw\", split_into=5)\n",
    "emails_to_trainfile(full_test, \"W3CGPT2/full.test.raw\", split_into=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Call for Training\n",
    "\n",
    "  - first: merge split up text files -> `cat full.train.raw.* > full.train.raw.all`\n",
    "\n",
    "`python3 run_language_modeling.py --train_data_file=W3CGPT2/full.train.raw.all --model_type=gpt2 --output_dir=W3CGPT2/lm --model_name_or_path=gpt2 --do_train --line_by_line --num_train_epochs=2`\n",
    "\n",
    " - `--line_by_line` indicates one sample per line to spearate e-mails, `\"\\n\"` inside e-mails converted to `\" \"`\n",
    " - perhaps use `--block_size=128/256/512` (rather than GPT-2's default of 1024) -> loose fewer tokens at the ends of long emails\n",
    " \n",
    " \n",
    "### 1.2 Load Trained\n",
    "\n",
    "should be as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2Model.from_pretrained('/W3CGPT2/lm/') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train one instance of W3CGPT-2 per author $X$ to become GPT-2$_X$\n",
    "\n",
    " - load W3CGPT2\n",
    " - get training files\n",
    " - call `run_language_modeling.py` with adequate parameters\n",
    " - run on LISA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training files, one per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GPT2_X/lm_Brian_McBride/nothing.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-224bb254aa7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"lm_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mauth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"lm_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mauth_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/nothing.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmail_ls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GPT2_X/lm_Brian_McBride/nothing.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_name = \"GPT2_X/\"\n",
    "for mail_ls in selection_train:\n",
    "    cur_auth = mail_ls[0].sender\n",
    "    auth_name = cur_auth.name.replace(\" \", \"_\")\n",
    "    \n",
    "    os.mkdir(folder_name + \"lm_\" + auth_name)\n",
    "    with open(folder_name + \"lm_\" + auth_name + \"/nothing.txt\", \"w\") as handle: pass\n",
    "    \n",
    "    for m in mail_ls:\n",
    "        with open(folder_name + auth_name + \".train.raw\", \"w\", encoding=\"utf-8\") as handle:\n",
    "                mail_str = m.body_raw.replace(\"\\n\", \"  \")\n",
    "                handle.write(mail_str)\n",
    "                handle.write(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
